# ğŸ§  Transformer From Scratch

A complete transformer architecture built from scratch using only NumPy - no PyTorch, no TensorFlow, just pure math and understanding.

Built to deeply understand how GPT, Llama, and Claude actually work under the hood.

---

## ğŸ¯ What I Built

| Component | Implementation | Purpose |
|-----------|----------------|---------|
| **Self-Attention** | Q, K, V matrices + scaled dot-product | Core innovation - lets model see relationships between all words |
| **Multi-Head Attention** | 4+ parallel attention heads | Learn different types of relationships (subject-verb, adjective-noun, etc.) |
| **Positional Encoding** | Sine/cosine position vectors | Give model position information (attention is position-blind) |
| **Feed-Forward Network** | 2-layer dense network with ReLU | Process attention output into useful representations |
| **Layer Normalization** | Mean/std normalization | Stabilize training across layers |
| **Residual Connections** | Skip connections | Enable deep networks (100+ layers) |
| **Full Transformer Block** | All above combined | The building block of GPT/Llama |
| **Text Generation** | Embedding + sampling | Generate sequences word by word |

---

## ğŸ“ Project Structure
```
transformer-from-scratch/
â”œâ”€â”€ attention.py       # Core transformer components
â”‚   â”œâ”€â”€ softmax()
â”‚   â”œâ”€â”€ self_attention()
â”‚   â”œâ”€â”€ multi_head_attention()
â”‚   â”œâ”€â”€ positional_encoding()
â”‚   â”œâ”€â”€ feed_forward()
â”‚   â”œâ”€â”€ layer_norm()
â”‚   â””â”€â”€ transformer_block()
â”‚
â””â”€â”€ generate.py        # Text generation
    â”œâ”€â”€ TransformerLM class
    â”œâ”€â”€ Vocabulary (11 words)
    â””â”€â”€ generate_text()
```

---

## ğŸš€ Usage

**Test individual components:**
```python
from attention import self_attention, multi_head_attention, positional_encoding

# Self-attention
x = np.random.randn(5, 8)  # 5 words, 8-dim embeddings
output = self_attention(x, W_Q, W_K, W_V)

# Multi-head attention
output = multi_head_attention(x, n_heads=4, d_model=8, d_k=6, d_v=6)

# Positional encoding
pos_enc = positional_encoding(seq_len=5, d_model=8)
```

**Generate text:**
```bash
python3 generate.py
```

**Output (with random weights):**
```
<START> mat fast fast the ran mat on fast
<START> dog sat <PAD> on <START> cat the fast
<START> ran <PAD> cat <PAD> the sat cat cat
```

*Note: Gibberish because weights are random (not trained). But the architecture works!*

---

## ğŸ§® The Math

**Self-Attention Formula:**
```
Attention(Q, K, V) = softmax(Q Ã— K^T / âˆšd_k) Ã— V
```

**Where:**
- Q (Query): "What am I looking for?"
- K (Key): "What do I contain?"
- V (Value): "What information do I provide?"

**Multi-Head Attention:**
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) Ã— W_O

where head_i = Attention(QÃ—W_Q_i, KÃ—W_K_i, VÃ—W_V_i)
```

**Positional Encoding:**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

---

## ğŸ—ï¸ Architecture
```
Input Text: "The cat sat"
        â†“
Tokenize: [3, 4, 5]
        â†“
Embeddings: (3, d_model)
        â†“
+ Positional Encoding
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block 1    â”‚
â”‚  â”œâ”€ Multi-Head Attn     â”‚
â”‚  â”œâ”€ Add & Norm          â”‚
â”‚  â”œâ”€ Feed Forward        â”‚
â”‚  â””â”€ Add & Norm          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block 2    â”‚
â”‚  â”œâ”€ Multi-Head Attn     â”‚
â”‚  â”œâ”€ Add & Norm          â”‚
â”‚  â”œâ”€ Feed Forward        â”‚
â”‚  â””â”€ Add & Norm          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Output Projection
        â†“
Softmax (probabilities)
        â†“
Next Word: "on"
```

---

## ğŸ“ What I Learned

**Why Attention?**
- RNNs process sequentially â†’ forget long-term context
- Attention sees ALL words at once â†’ captures any relationship

**Why Multi-Head?**
- One head can only learn one pattern
- Multiple heads learn different relationships in parallel

**Why Positional Encoding?**
- Attention is mathematically position-blind
- "Dog bites man" vs "Man bites dog" look identical without position info

**Why Residual Connections?**
- Deep networks have vanishing gradient problem
- Skip connections let gradients flow directly through
- Allows stacking 100+ layers (GPT-3 has 96 blocks)

---

## ğŸ”¥ Key Insights

**This is the SAME architecture as:**
- GPT-3, GPT-4 (OpenAI)
- Llama 3.2 (Meta) 
- Claude (Anthropic)
- Mistral, Gemini, etc.

**Differences:**
- **Mine:** 11 words, 2 blocks, random weights
- **GPT-4:** 100K+ words, 96+ blocks, trained on trillions of tokens

**Same fundamental structure.** I now understand what happens inside these models.

---

## ğŸ“Š Components Breakdown

| File | Lines | What It Does |
|------|-------|--------------|
| `attention.py` | ~150 | All transformer building blocks |
| `generate.py` | ~80 | Text generation with simple vocabulary |

**Total:** Built a working transformer in ~230 lines of NumPy code.

---

## ğŸ› ï¸ Technical Details

- **Language:** Python 3
- **Dependencies:** NumPy only
- **Model Size:** 
  - Vocabulary: 11 tokens
  - d_model: 16 (embedding dimension)
  - n_heads: 4 (attention heads)
  - n_blocks: 2 (transformer layers)
  - Parameters: ~3,500 (vs GPT-3's 175 billion)

---

## ğŸš§ What's NOT Implemented

- **Training loop** - Weights are random, not learned
- **Backpropagation** - No gradient computation
- **Optimization** - No Adam, no learning rate schedules
- **Real vocabulary** - Only 11 words, not 50K+ tokens
- **Scaling** - No GPU support, no batching optimizations

**These are engineering additions.** The core architecture is complete.

---

## ğŸ¯ Next Steps

To make this a REAL language model:
1. Load real training data (Wikipedia, books, etc.)
2. Implement backpropagation for all layers
3. Add cross-entropy loss function
4. Train with Adam optimizer
5. Scale up (more blocks, larger vocabulary, GPU)

**Or:** Use this understanding to work with existing models (Hugging Face, fine-tuning, etc.)

---

## ğŸ“š Resources

**Papers:**
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original transformer paper
- [BERT](https://arxiv.org/abs/1810.04805) - Bidirectional transformers
- [GPT-3](https://arxiv.org/abs/2005.14165) - Language models at scale

**What I also built:**
- [ml-from-scratch](https://github.com/kush-3/ml-from-scratch) - Neural networks from scratch (97.58% MNIST)
- [paper-explainer](https://github.com/kush-3/paper-explainer) - AI-powered arXiv paper summarizer

---

## ğŸ“ License

MIT

---

**Built by [Kush Patel](https://github.com/kush-3) to deeply understand transformers, not just use them.**

*"The best way to understand something is to build it from scratch."*